---
layout: post
title: "Uninitialize Data Access."
subtitle: "Uninitialice Data Access notes from OST2 course."
date: 2025-12-23 09:00:00 +0000
categories: ['Past Blogs', 'Binary Exploitation']
tags: ['memory-corruption', 'exploits']
author: German Sanmi
---


### 1. Definition.

#### 1.1. Preface. Initialize vs Uninitialize data.

First, let's introduce two memory regions in which the program data-storage happens at runtime, the stack and the heap.

- The *stack* is a LIFO structure in which fixed-size data within the function stackframe's owner scope gets stored.

- The *heap* is an amount of memory available to get chunk-divided and manually allocated as required at runtime by the program generally for runtime-defined size data.


<br>

```
High addresses (0x7FFF...)
┌─────────────────────────────────┐
│                                 │
│            STACK                │
│                                 │
│   - Local variables             │
│   - Function parameters         │
│   - Return addresses            │
│   - Saved frame pointers        │
│                                 │
│      ↓ grows downward ↓         │
├─────────────────────────────────┤
│                                 │
│                                 │
│                                 │
│        (unmapped space)         │
│                                 │
│                                 │
│                                 │
├─────────────────────────────────┤
│      ↑ grows upward ↑           │
│                                 │
│            HEAP                 │
│                                 │
│   - malloc()/calloc() chunks    │
│   - Dynamic allocations         │
│   - Heap metadata               │
│                                 │
.                                 .
.                                 .
.                                 .

Low addresses (0x0000...)
```

<br>

In the case of the stack, the allocations happens when the compiler expands the stackframe to make space for new variables by pulling the RSP register down (SUB rsp, 0x20; for example). Then it starts by copying or reading data from that expansion (MOV eax, dword ptr \[rbp-4\]). 

<br>

```
BEFORE: SUB rsp, 0x20             AFTER: SUB rsp, 0x20
                    
┌─────────────────────┐           ┌─────────────────────┐
│   Return Address    │           │   Return Address    │
├─────────────────────┤           ├─────────────────────┤
│     Saved RBP       │           │     Saved RBP       │
├─────────────────────┤◄─ RBP,RSP ├─────────────────────┤ ◄─ RBP
│                     │           │                     │
│                     │           │  Local Variables    │
│                     │           │     (0x20 bytes)    │
│    (unmapped)       │           │                     │
│                     │           ├─────────────────────┤ ◄─ RSP
│                     │           │                     │
│                     │           │    (unmapped)       │
└─────────────────────┘           └─────────────────────┘
Low addresses                     Low addresses

    RSP pulled down → space alloced in the stack 
```

<br>

In the case of the heap, a manual request of x bytes of data is sent and then, a fixed-size memory chunk gets alloced and usually a pointer with the first address of this chunk is stored in the stack.

<br>

```
BEFORE: malloc(0x20)              AFTER: malloc(0x20)

HEAP:                             HEAP:
┌─────────────────────┐           ┌─────────────────────┐
│                     │           │    Heap Metadata    │
│                     │           ├─────────────────────┤◄─ returned ptr, stored as a local variable in the stack
│   (available)       │           │                     │
│                     │           │   Allocated Chunk   │ 
│                     │           │     (0x20 bytes)    │
│                     │           │                     │
└─────────────────────┘           ├─────────────────────┤
                                  │    (available)      │
                                  └─────────────────────┘
```

<br>

This is the way in which data gets alloced in memory at runtime when a program is executing. This essentially means that the OS reserves space for the data that is about to be stored. Observe that the stack and the heap always posees data, this means that the alloced memory regions are very likely to posees data even before to be alloced again for a reuse. The memory region which has no data placed in since it was alloced is labeled as *uninitialized*, then, when the data is placed in this is *initialize data*. 

This distinction about data present in a memory region before use it and data written into a memory region is important to understand the vulnerability we are about to explain.

<br>

#### 1.2. UDA definition.

UDA, Uninitialize Data Access, is a vulnerability which arises when a program reads from memory that was allocated but never explicitly set to a known value, this is; when a program access to uninitialized data.

As we say before, when a program needs to write into memory it first needs to allocate space, when this space gets alloced then the program uses this memory-chunk placing data into it and once this process finish and this chunk serves no-purpouse (if the programmer has good "programming habits") this chunk gets presumibly *freed* which means that this block of data is labeled as *available* again to be recycled. However, free does not mean clear or zero-cleared, this means; the previously written data does not get erased from a freed chunk (this is called "garbage data"), is just released from the program's allocator perspective to be reused later but, in the mean while.

This is the key to understand this vulnerability, when a program request a memory chunk, this uninitilize structure offers data that can still be accesed, read it and retrieved.

Is worth to note that exploiting this kind of vulnerabilities is not trivial, since a good understanding of memory layout is needed in order to understand what is being accesed when an uninitialize variable is used as the following example shows.

<br>

### 2. Examples.

#### 2.1. Trivial Example 1: uad1.c

Let's consider the following code:

```c
// Uninitialized Data Access 1 (stack) - uda1.c
#include <stdio.h>
#include <string.h>

void uda_func(int * p){
    int i;
    printf("We all know that %x is leet, right?\n", i);
}

void acid_setter_func(int * p){
    int i = *p;
    printf("We all know that %x is leet, right?\n", i);
}

int main(int argc, char * argv[]){
    char buf[8] = {0};
    int i = 0x1337;
    printf("argc = %d\n", argc);
    if(argc > 1){
        strcpy(buf, argv[1]);
        i = *(int *)(&buf[0]);
    }
    if(buf[0]){
        acid_setter_func(&i);
    }
    uda_func(&i);
    return 0;
}
```

This code doesn't have a explicty security issue (despite the Stack-Buffer Overflow in which we are not interested in) but is usefull to understand how recently used memory is accessible thorugh uninitialized data.

If we track the execution-flow of the code we can see that first, we declare a zero-cleared stack buffer and a signed integer.

Then, if the program has been executed among with a cli-parameter argument, it store that parameter within the buffer and in a apparently convoluted way, store this value in i integer. The way in which this assignation is performed ensures the control of an integer-width value.

- First, take the address of buf[0].
- Then cast it as a pointer to an integer, this means that now, the address points to a 4-bytes width data (instead of 1 byte).
- Then dereference the contents of that pointer (4 bytes) and store it in *i*.

Now, if buf[0] is not null, then it calls to *acid_setter_func()* which declares and initialites a variable which later is used in a printf function.

Later, calls *uda_func()* which barely do the same but with uninitialize data, only declares *i* and then proceeds to use it.

If we compile the code and execute the following binary we will get the following results:

```bash
$ ./test AAAA
argc = 2
We all know that 41414141 is leet, right?
We all know that 41414141 is leet, right?
```

We can see that the second line, provided by *uda_func()* is accessing the data stored in the memory allocated space for an uninitialized declared variable.

Is worth to note that UDA depends on a good memory layout understanding in order to comprehend what is being accesed. The reason why *i* variables holds the same value in this specific case is due to layout similarity and call-proximity in code of *uda_func()* and *acid_setter_func()* as is represented in the following diagram: 

```
HIGHER ADDRESSES
│
▼

┌────────────────────────────────┐
│          main() frame          │
└────────────────────────────────┘


STEP 1: call acid_setter_func()

┌────────────────────────────────┐
│          main() frame          │
├────────────────────────────────┤
│    acid_setter_func() frame    │
│                                │
│    int i = 0x41414141  ← WRITE │
└────────────────────────────────┘


STEP 2: return — frame retracts

┌────────────────────────────────┐
│          main() frame          │
├ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┤
│                                │
│    (0x41414141 still here)     │
│                                │
└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┘


STEP 3: call uda_func()

┌────────────────────────────────┐
│          main() frame          │
├────────────────────────────────┤
│       uda_func() frame         │
│                                │
│    int i;          ← READ      │
│    (gets 0x41414141)           │
└────────────────────────────────┘

│
▼
LOWER ADDRESSES
```

<br>

In order to understand it better, lets take the dissasembly for both functions:

- First, the dissasembly for *acid_setter_func()*:

    ```bash
    $ r2 -e asm.sub.var=false -A -qc "aaa; s sym.acid_setter_func;pdf" test
    Warning: run r2 with -e bin.cache=true to fix relocations in disassembly
                ; CALL XREF from main @ 0x126c
    ┌ 53: sym.acid_setter_func (int64_t arg1);
    │           ; var int64_t var_18h @ rbp-0x18
    │           ; var int64_t var_4h @ rbp-0x4
    │           ; arg int64_t arg1 @ rdi
    │           0x000011b5      f30f1efa       endbr64
    │           0x000011b9      55             push rbp
    │           0x000011ba      4889e5         mov rbp, rsp
    │           0x000011bd      4883ec20       sub rsp, 0x20
    │           0x000011c1      48897de8       mov qword [rbp - 0x18], rdi ; arg1
    │           0x000011c5      488b45e8       mov rax, qword [rbp - 0x18]
    │           0x000011c9      8b00           mov eax, dword [rax]
    │           0x000011cb      8945fc         mov dword [rbp - 4], eax
    │           0x000011ce      8b45fc         mov eax, dword [rbp - 4]
    │           0x000011d1      89c6           mov esi, eax
    │           0x000011d3      488d052e0e00.  lea rax, str.We_all_know_that__x_is_leet__right__n ; 0x2008 ; "We all know that %x is leet, right?\n"
    │           0x000011da      4889c7         mov rdi, rax                ; const char *format
    │           0x000011dd      b800000000     mov eax, 0
    │           0x000011e2      e8a9feffff     call sym.imp.printf         ; int printf(const char *format)
    │           0x000011e7      90             nop
    │           0x000011e8      c9             leave
    └           0x000011e9      c3             ret
    ```
    
    We can intuit the stack layout from the stackframe prologue, it expands the stack by 0x20:

    ```assem
    push rbp
    mov rbp, rsp
    sub rsp, 0x20
    ```

    Then, since this is GCC compiler in Unix-like system, it follows System V convention, it pass rdi (parameter *i*) to the location "rbp-0x18" and from there it ends up in "rbp-4":

    ```assem
    mov qword [rbp - 0x18], rdi ; arg1
    mov rax, qword [rbp - 0x18]
    mov eax, dword [rax]
    mov dword [rbp - 4], eax
    mov eax, dword [rbp - 4]
    ```

    Lets note that when the function finish, before RET, LEAVE instruction is executed which restore stackframe, LEAVE is equivalent to:

    ```assem
    mov rsp, rbp    ; restore RSP to where it was before the function
    pop rbp         ; restore caller's RBP
    ```
    
- Lets now check the *uda_func()* dissasembly:

    ```assem
    $ r2 -e asm.sub.var=false -A -qc "aaa; s sym.uda_func;pdf" test
    Warning: run r2 with -e bin.cache=true to fix relocations in disassembly
                ; CALL XREF from main @ 0x1278
    ┌ 44: sym.uda_func (int64_t arg1);
    │           ; var int64_t var_18h @ rbp-0x18
    │           ; var int64_t var_4h @ rbp-0x4
    │           ; arg int64_t arg1 @ rdi
    │           0x00001189      f30f1efa       endbr64
    │           0x0000118d      55             push rbp
    │           0x0000118e      4889e5         mov rbp, rsp
    │           0x00001191      4883ec20       sub rsp, 0x20
    │           0x00001195      48897de8       mov qword [rbp - 0x18], rdi ; arg1
    │           0x00001199      8b45fc         mov eax, dword [rbp - 4]
    │           0x0000119c      89c6           mov esi, eax

    [...]
    ```
    
    The code continues but we don't need more than what we have. First, the stack expands 0x20 (the same as the previous function) and the parameter ends again in "rbp-4" to later move this to ESI (register-passed for second parameter through System V convention) to later pass it to printf() function.

    This means that is essentially accessing the 32-bits data which were written by the previous function. This result in an UDA vulnerability.

    <br>

#### 2.2. Trivial Example 2: uda2.c

Consider now the following code:


```c
// Uninitialized Data Access 1 (stack) - uda1.c
#include <stdio.h>
#include <string.h>

int uda_func(int * p){
        int i;
        printf("We all know that %x is leet, right?\n", i);
        return i;
}

void acid_setter_func(int * p){
        int i = *p;
        printf("We all know that %x is leet, right?\n", i);
}

int main(int argc, char * argv[]){
        char buf[8] = {0};
        int i = 0x1337;
        printf("argc = %d\n", argc);
        if(argc > 1){
                strcpy(buf, argv[1]);
                i = *(int *)(&buf[0]);
        }
        if(buf[0]){
                acid_setter_func(&i);
        }
        memcpy(buf, argv[1], uda_func(&i));
        return 0;
}
```

This code is pretty the same than above with the small difference that now *uda_func()* is returning *i* and this value is being implemented as the len parameter of a memcpy() call.

Now is easy to understand that this suppose a bigger danger than the previous example.

Following the flow explained in the example above, the user-controlled data set in memory by *acid_setter_func()* is then retrieved by *uda_func()* and placed in a memcpy() operation potentially triggering a SBO.

<br>

#### 2.3. Trivial Example 3. Heap.c

Consider now the following code:

```c
// Uninitialized Data Access 2 (heap) - uda2.c
#include <stdio.h>
#include <string.h>
#include <stdlib.h>

#define BUF_SIZE 1024

void opt_realloc(char ** buf1, char ** buf2){
    free(*buf1);
    free(*buf2);
    *buf2 = malloc(BUF_SIZE); //XENO: Note, I switched the order of allocs
    *buf1 = malloc(BUF_SIZE); //XENO: This was based on system-specific knowledge
    printf("buf1 addr = %p, buf2 addr = %p\n", *buf1, *buf2);
}

int main(int argc, char * argv[]){
    char * buf1 = malloc(BUF_SIZE);
    char * buf2 = malloc(BUF_SIZE);
    int * i = (int *)buf1;
    printf("buf1 addr = %p, buf2 addr = %p\n", buf1, buf2);
    printf("argc = %d\n", argc);
    if(argc > 1){
        strcpy(buf1, argv[1]);
        memset(buf2, '!', BUF_SIZE);
    }
    if(buf1[0]){
        opt_realloc(&buf1, &buf2);
    }
    for(unsigned int j = 0; j < strlen(argv[1])/4; j++){
        printf("At %p+%d:\t %x\n", i, j*4, *(int *)(i+j));
    }
    i = (int *)buf2;
    printf("\n");
    for(unsigned int j = 0; j < strlen(argv[1])/4; j++){
        printf("At %p+%d:\t %x\n", i, j*4, *(int *)(i+j));
    }
    printf("At the end of the day, the important thing is: %x\n", *(int *)&buf1[16]);
    return 0;
}
```


This time we are involving heap-memory region. 

First, we define two pointers to which we assignate a memory chunk with malloc:

```c
char * buf1 = malloc(BUF_SIZE);
char * buf2 = malloc(BUF_SIZE);
```

Then we cast *buf1* to pointer to a integer:

```c
int * i = (int *)buf1;
```

And if we, along with the program, pass a cli-parameter, then the buffer gets fulfilled amd then we call the opt_realloc() function which is a wrapper around malloc in which we free a pointer and then alloc and assign a memory chunk to this pointer:

```c
void opt_realloc(char ** buf1, char ** buf2){
    free(*buf1);
    free(*buf2);
    *buf2 = malloc(BUF_SIZE); //XENO: Note, I switched the order of allocs
    *buf1 = malloc(BUF_SIZE); //XENO: This was based on system-specific knowledge
    printf("buf1 addr = %p, buf2 addr = %p\n", *buf1, *buf2);
}
//...
if(buf1[0]){
    opt_realloc(&buf1, &buf2);
}
```

Due to how the system behaves, in this case once you free a memroy chunk and inmediately assign it, you have assigned the same memory chunk as before, this means that, despite gets realloced, *buf1* and *buf2* has no changed at this point.

Then, we simply read from this uninitialized structures:

```c
for(unsigned int j = 0; j < strlen(argv[1])/4; j++){
    printf("At %p+%d:\t %x\n", i, j*4, *(int *)(i+j));
}
i = (int *)buf2;
printf("\n");
for(unsigned int j = 0; j < strlen(argv[1])/4; j++){
    printf("At %p+%d:\t %x\n", i, j*4, *(int *)(i+j));
}
printf("At the end of the day, the important thing is: %x\n", *(int *)&buf1[16]);
return 0;
```

<br>

Reach this point, if we compile the binary and execute it we obtain the following output:

```text
$ ./test "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA"
buf1 addr = 0x581a809a32a0, buf2 addr = 0x581a809a36b0
argc = 2
buf1 addr = 0x581a809a32a0, buf2 addr = 0x581a809a36b0
At 0x581a809a32a0+0:     81a809a3
At 0x581a809a32a0+4:     5
At 0x581a809a32a0+8:     0
At 0x581a809a32a0+12:    0         ; Some of the previous data gets has been erased
At 0x581a809a32a0+16:    41414141  ; AAAA
At 0x581a809a32a0+20:    41414141
At 0x581a809a32a0+24:    41414141
At 0x581a809a32a0+28:    41414141
At 0x581a809a32a0+32:    41414141
At 0x581a809a32a0+36:    41414141
At 0x581a809a32a0+40:    41414141
At 0x581a809a32a0+44:    41414141
At 0x581a809a32a0+48:    41414141

At 0x581a809a36b0+0:     1323b03
At 0x581a809a36b0+4:     581f
At 0x581a809a36b0+8:     0
At 0x581a809a36b0+12:    0
At 0x581a809a36b0+16:    21212121
At 0x581a809a36b0+20:    21212121
At 0x581a809a36b0+24:    21212121
At 0x581a809a36b0+28:    21212121
At 0x581a809a36b0+32:    21212121
At 0x581a809a36b0+36:    21212121
At 0x581a809a36b0+40:    21212121
At 0x581a809a36b0+44:    21212121
At 0x581a809a36b0+48:    21212121
At the end of the day, the important thing is: 41414141
```

We can see that, despite being both buffer freed, the previous allocated data still accesible, this is essentially an UDA vulnerability.

<br>

### 3. Common Roots.

Among the common causes that leads to a UDA vulnerability are:

- Not initializing local variables at declaration time.
- No initializing heap data at allocation time.
- Only partially initializing structs & objects.
- Accidental failure to initialize down an uncommon control flow path.

If a codebase uses a lot function pointers / callbacks / indirect control flow, it is far more difficult for a human auditor to confidently assess. In such cases tooling and automated analysis (e.g. MemorySanitizer and/or a Fuzzer) should be brought to bear.

<br>

### 4. Exercises.

#### 4.1. CVE-2022-1809.

Radare2 is a reverse engineering tool, often used to analyze potentially-malicious binaries such as malware, therefore all values that come from the binary need to be treated as ACID.

Consider the following C code:

```c
//////////////////////////////////////////////////////////////////////
//XENO: Structure that isn't completely initialized
//////////////////////////////////////////////////////////////////////
/* vtables */
typedef struct {
	RAnal *anal;
	RAnalCPPABI abi;
	ut8 word_size;
	bool (*read_addr) (RAnal *anal, ut64 addr, ut64 *buf);
} RVTableContext;

//////////////////////////////////////////////////////////////////////
//XENO: Part of the path where incomplete initialized occurs
//////////////////////////////////////////////////////////////////////

//XENO: assume the following fields are ACID based on a malicious ACID binary under analysis:
//XENO: anal->config->bits, anal->cur->arch

R_API bool r_anal_vtable_begin(RAnal *anal, RVTableContext *context) {
	context->anal = anal;
	context->abi = anal->cxxabi;
	context->word_size = (ut8) (anal->config->bits / 8);
	const bool is_arm = anal->cur->arch && r_str_startswith (anal->cur->arch, "arm");
	if (is_arm && context->word_size < 4) {
		context->word_size = 4;
	}
	const bool be = anal->config->big_endian;
	switch (context->word_size) {
	case 1:
		context->read_addr = be? vtable_read_addr_be8 : vtable_read_addr_le8;
		break;
	case 2:
		context->read_addr = be? vtable_read_addr_be16 : vtable_read_addr_le16;
		break;
	case 4:
		context->read_addr = be? vtable_read_addr_be32 : vtable_read_addr_le32;
		break;
	case 8:
		context->read_addr = be? vtable_read_addr_be64 : vtable_read_addr_le64;
		break;
	default:
		return false;
	}
	return true;
}

//////////////////////////////////////////////////////////////////////
//XENO: Part of the path where uninitialized access occurs eventually
//////////////////////////////////////////////////////////////////////


R_API void r_anal_list_vtables(RAnal *anal, int rad) {
	RVTableContext context;
	r_anal_vtable_begin (anal, &context);

	const char *noMethodName = "No Name found";
	RVTableMethodInfo *curMethod;
	RListIter *vtableIter;
	RVTableInfo *table;

	RList *vtables = r_anal_vtable_search (&context);
//XENO: snip
}

R_API RList *r_anal_vtable_search(RVTableContext *context) {
	RAnal *anal = context->anal;
	if (!anal) {
		return NULL;
	}

	RList *vtables = r_list_newf ((RListFree)r_anal_vtable_info_free);
	if (!vtables) {
		return NULL;
	}

	RList *sections = anal->binb.get_sections (anal->binb.bin);
	if (!sections) {
		r_list_free (vtables);
		return NULL;
	}

	r_cons_break_push (NULL, NULL);

	RListIter *iter;
	RBinSection *section;
	r_list_foreach (sections, iter, section) {
		if (r_cons_is_breaked ()) {
			break;
		}

		if (!vtable_section_can_contain_vtables (section)) {
			continue;
		}

		ut64 startAddress = section->vaddr;
		ut64 endAddress = startAddress + (section->vsize) - context->word_size;
		ut64 ss = endAddress - startAddress;
		if (ss > ST32_MAX) {
			break;
		}
		while (startAddress <= endAddress) {
			if (r_cons_is_breaked ()) {
				break;
			}
			if (!anal->iob.is_valid_offset (anal->iob.io, startAddress, 0)) {
				break;
			}

			if (vtable_is_addr_vtable_start (context, section, startAddress)) {
				RVTableInfo *vtable = r_anal_vtable_parse_at (context, startAddress);
				if (vtable) {
					r_list_append (vtables, vtable);
					ut64 size = r_anal_vtable_info_get_size (context, vtable);
					if (size > 0) {
						startAddress += size;
						continue;
					}
				}
			}
			startAddress += context->word_size;
		}
	}
//XENO: snip
}

static bool vtable_is_addr_vtable_start(RVTableContext *context, RBinSection *section, ut64 curAddress) {
	if (context->abi == R_ANAL_CPP_ABI_MSVC) {
		return vtable_is_addr_vtable_start_msvc (context, curAddress);
	}
	if (context->abi == R_ANAL_CPP_ABI_ITANIUM) {
		return vtable_is_addr_vtable_start_itanium (context, section, curAddress);
	}
	r_return_val_if_reached (false);
	return false;
}

static bool vtable_is_addr_vtable_start_msvc(RVTableContext *context, ut64 curAddress) {
	RAnalRef *xref;
	RListIter *xrefIter;

	if (!curAddress || curAddress == UT64_MAX) {
		return false;
	}
	if (curAddress && !vtable_is_value_in_text_section (context, curAddress, NULL)) {
		return false;
	}
//XENO: snip
}

static bool vtable_is_value_in_text_section(RVTableContext *context, ut64 curAddress, ut64 *value) {
	//value at the current address
	ut64 curAddressValue;
	if (!context->read_addr (context->anal, curAddress, &curAddressValue)) {
		return false;
	}
	//if the value is in text section
	bool ret = vtable_addr_in_text_section (context, curAddressValue);
	if (value) {
		*value = curAddressValue;
	}
	return ret;
}
```

<br>

Let's reorganize de code to understand it better:

- First, *r_anal_list_vtables()* calls both *r_anal_vtable_begin()* which initializes the previously declared variable *context* and *r_anal_vtable_search()* also with *context* passed as parameter. Let's observe that the parameter *anal->config->bits* is user-controlled, and thus, *context->word_size* also is and due to this, there is a margin in which the *read_addr* field does not get initialized:

    ```c
    R_API bool r_anal_vtable_begin(RAnal *anal, RVTableContext *context) {
	context->anal = anal;
	context->abi = anal->cxxabi;
	context->word_size = (ut8) (anal->config->bits / 8);
    //...
	switch (context->word_size) {
	//Depending on the case, context->read_addr get assigned a value except in default case.
	default:
		return false;
	}
	return true;
    }

    R_API void r_anal_list_vtables(RAnal *anal, int rad) {
	RVTableContext context;
	r_anal_vtable_begin (anal, &context);
    //...
    }
    ```

- Then, *r_anal_vtable_search()* calls *vtable_is_addr_vtable_start()* which calls *vtable_is_addr_vtable_start_msvc()* as well and this one to *vtable_is_value_in_text_section()* which attemps to read from *context->read_addr*:

    ```c
    static bool vtable_is_value_in_text_section(RVTableContext *context, ut64 curAddress, ut64 *value) {
	//value at the current address
	ut64 curAddressValue;
	if (!context->read_addr (context->anal, curAddress, &curAddressValue)) {
		return false;
	}
    //...
    ```

This is an unintialized memory region being readed, leading to Uninitialized Data Access vulnerability.

<br>

#### 4.2. CVE-2021-3608.

Consider the following C code:

```c
//////////////////////////////////////////////////////////////////////
//XENO: Structure that isn't completely initialized
//////////////////////////////////////////////////////////////////////

typedef struct PvrdmaRing {
    char name[MAX_RING_NAME_SZ];
    PCIDevice *dev;
    uint32_t max_elems;
    size_t elem_sz;
    PvrdmaRingState *ring_state; /* used only for unmap */
    int npages;
    void **pages;
} PvrdmaRing;

//////////////////////////////////////////////////////////////////////
//XENO: Part of the path where incomplete initialized occurs AND uninitialized usage occurs
//////////////////////////////////////////////////////////////////////

//XENO: Assume dir_addr and num_pages are ACID
//XENO: And assume that if the 2nd argument to rdma_pci_dma_map() is ACID
//XENO: then it's basically just mapping more ACID data/structs into memory
static int init_dev_ring(PvrdmaRing *ring, PvrdmaRingState **ring_state,
                         const char *name, PCIDevice *pci_dev,
                         dma_addr_t dir_addr, uint32_t num_pages)
{
    uint64_t *dir, *tbl;
    int rc = 0;

    dir = rdma_pci_dma_map(pci_dev, dir_addr, TARGET_PAGE_SIZE);
    if (!dir) {
        rdma_error_report("Failed to map to page directory (ring %s)", name);
        rc = -ENOMEM;
        goto out;
    }
    tbl = rdma_pci_dma_map(pci_dev, dir[0], TARGET_PAGE_SIZE);
    if (!tbl) {
        rdma_error_report("Failed to map to page table (ring %s)", name);
        rc = -ENOMEM;
        goto out_free_dir;
    }

    *ring_state = rdma_pci_dma_map(pci_dev, tbl[0], TARGET_PAGE_SIZE);
    if (!*ring_state) {
        rdma_error_report("Failed to map to ring state (ring %s)", name);
        rc = -ENOMEM;
        goto out_free_tbl;
    }
    /* RX ring is the second */
    (*ring_state)++;
    rc = pvrdma_ring_init(ring, name, pci_dev,
                          (PvrdmaRingState *)*ring_state,
                          (num_pages - 1) * TARGET_PAGE_SIZE /
                          sizeof(struct pvrdma_cqne),
                          sizeof(struct pvrdma_cqne),
                          (dma_addr_t *)&tbl[1], (dma_addr_t)num_pages - 1);
    if (rc) {
        rc = -ENOMEM;
        goto out_free_ring_state;
    }

    goto out_free_tbl;

out_free_ring_state:
    rdma_pci_dma_unmap(pci_dev, *ring_state, TARGET_PAGE_SIZE);

out_free_tbl:
    rdma_pci_dma_unmap(pci_dev, tbl, TARGET_PAGE_SIZE);

out_free_dir:
    rdma_pci_dma_unmap(pci_dev, dir, TARGET_PAGE_SIZE);

out:
    return rc;
}

int pvrdma_ring_init(PvrdmaRing *ring, const char *name, PCIDevice *dev,
                     PvrdmaRingState *ring_state, uint32_t max_elems,
                     size_t elem_sz, dma_addr_t *tbl, uint32_t npages)
{
    int i;
    int rc = 0;

    pstrcpy(ring->name, MAX_RING_NAME_SZ, name);
    ring->dev = dev;
    ring->ring_state = ring_state;
    ring->max_elems = max_elems;
    ring->elem_sz = elem_sz;
    /* TODO: Give a moment to think if we want to redo driver settings
    qatomic_set(&ring->ring_state->prod_tail, 0);
    qatomic_set(&ring->ring_state->cons_head, 0);
    */
    ring->npages = npages;
    ring->pages = g_malloc(npages * sizeof(void *)); //XENO: array of npages pointers

    for (i = 0; i < npages; i++) {
        if (!tbl[i]) {
            rdma_error_report("npages=%d but tbl[%d] is NULL", npages, i);
            continue;
        }

        ring->pages[i] = rdma_pci_dma_map(dev, tbl[i], TARGET_PAGE_SIZE);
        if (!ring->pages[i]) {
            rc = -ENOMEM;
            rdma_error_report("Failed to map to page %d in ring %s", i, name);
            goto out_free;
        }
        memset(ring->pages[i], 0, TARGET_PAGE_SIZE);
    }

    goto out;

out_free:
    while (i--) {
        rdma_pci_dma_unmap(dev, ring->pages[i], TARGET_PAGE_SIZE);
    }
    g_free(ring->pages);

out:
    return rc;
}
```

Let's observe that *dir_addr* being user-controlled implies *tbl* being user-controlled (along with *num_pages*) which both are passed as parameters to *pvrdma_ring_init()*

In this function, *num_pages* assumes the form of *npages* and *tbl* as *tbl* pointer, as two separate user-controller values, and there exists the following loop:

```c
for (i = 0; i < npages; i++) {
    if (!tbl[i]) {
        rdma_error_report("npages=%d but tbl[%d] is NULL", npages, i);
        continue;
    }
    //...
```

Observe that there are checking the same number of *tbl* entries and *npages*, however, since this two values are user-controlled, eventually a huge *npages* value and a small *tbl* array can lead to the program try to access undefenied, not-initialize, *tbl* entires, leading to Uninitialize Data Access vulnerability.

<br>

#### 4.3. CVE-2022-26712.

Processes on macOS and iOS can use XPC (Mac OS X inter-Process Communication) to send data between each other. XPC (cross-process communication) is Apple's IPC mechanism for communication between processes, heavily used for privilege separation. When you send a message via XPC, you're crossing process boundaries—different address spaces—so the data must be serialized (convert data to a stream of bytes to be reconstructed later). 

Consider the following code:

```c
xpc_object_t content = xpc_dictionary_get_value(req, "source");
size_t count = xpc_array_get_count(content); //XENO: count SACI, based on number of array elements sent
size_t *descriptors = malloc(sizeof(size_t) * 4 * count);
size_t *accessBeginPointer = &descriptors[count * 0], *accessDataLength = &descriptors[count * 1], *mappedBaseAddress = &descriptors[count * 2], *mappedLength = &descriptors[count * 3];

for(size_t i = 0; i < count; i++) {
    accessBeginPointer[i] = accessDataLength[i] = mappedBaseAddress[i] = mappedLength[i] = 0;

    xpc_object_t chunk = xpc_array_get_value(content, i);

    if(xpc_get_type(chunk) == XPC_TYPE_DATA) { /*...*/ }
    else if(xpc_get_type(chunk) == XPC_TYPE_SHMEM) {
    xpc_object_t map = xpc_array_get_value(chunk, 0);
    size_t offset = min(xpc_array_get_uint64(chunk, 1), 0xFFF), //XENO: offset SACI
    size = xpc_array_get_uint64(chunk, 2);                      //XENO: size ACID

    size_t mapped_address;
    size_t mapped_size = xpc_shmem_map(map, &mapped_address);   //XENO: mapped_size ACID

    size = min(size, mapped_size - offset);
    // ...
    }
}
// ...
// cleanup
for(size_t index = 0; index < count; index++) {
    if(mappedLength[index]) {
    munmap(mappedBaseAddress[index], mappedLength[index]);
    }
}
free(descriptors);
```

Lets observe the following line:

```c
size_t *descriptors = malloc(sizeof(size_t) * 4 * count);
```

Considering that count is in some form user-controlled and integer overflow can be present in the mathematic operation within the malloc passed parameter leading to an underallocation for *descriptor* pointer.

Then, some pointers are defined through *descriptor* pointer:

```c
size_t *accessBeginPointer = &descriptors[count * 0], 
        *accessDataLength = &descriptors[count * 1], 
        *mappedBaseAddress = &descriptors[count * 2], 
        *mappedLength = &descriptors[count * 3]; //<-- Interesting
```

Observe that, despite some assignations are being made, the descriptors pointer still not-initialize memery regions.

Thus, below, in the for loop, since count is a big value, eventually Out-Of-Bounds write and read happens:

```c
for(size_t index = 0; index < count; index++) {
    if(mappedLength[index]) {
    //...
```

<br>

#### 4.4. CVE-2022-29968.

Consider the following C code:

```c
//////////////////////////////////////////////////////////////////////
//XENO: Structure that isn't completely initialized
//////////////////////////////////////////////////////////////////////
struct io_kiocb {
	union {
		struct file		*file;
		struct io_rw		rw;
		struct io_poll_iocb	poll;
		struct io_poll_update	poll_update;
		struct io_accept	accept;
		struct io_sync		sync;
		struct io_cancel	cancel;
		struct io_timeout	timeout;
		struct io_timeout_rem	timeout_rem;
		struct io_connect	connect;
		struct io_sr_msg	sr_msg;
		struct io_open		open;
		struct io_close		close;
		struct io_rsrc_update	rsrc_update;
		struct io_fadvise	fadvise;
		struct io_madvise	madvise;
		struct io_epoll		epoll;
		struct io_splice	splice;
		struct io_provide_buf	pbuf;
		struct io_statx		statx;
		struct io_shutdown	shutdown;
		struct io_rename	rename;
		struct io_unlink	unlink;
		struct io_mkdir		mkdir;
		struct io_symlink	symlink;
		struct io_hardlink	hardlink;
		struct io_msg		msg;
	};

	u8				opcode;
	u8				iopoll_completed;
	u16				buf_index;
	unsigned int			flags;

	u64				user_data;
	u32				result;
	union {
		u32			cflags;
		int			fd;
	};

	struct io_ring_ctx		*ctx;
	struct task_struct		*task;

	struct percpu_ref		*fixed_rsrc_refs;
	struct io_mapped_ubuf		*imu;

	union {
		struct io_wq_work_node	comp_list;
		int apoll_events;
	};
	atomic_t			refs;
	atomic_t			poll_refs;
	struct io_task_work		io_task_work;
	struct hlist_node		hash_node;
	struct async_poll		*apoll;
	void				*async_data;
	struct io_buffer		*kbuf;
	struct io_kiocb			*link;
	const struct cred		*creds;
	struct io_wq_work		work;
};

//////////////////////////////////////////////////////////////////////
//XENO: Part of the path where incomplete initialization occurs
//////////////////////////////////////////////////////////////////////

static int io_rw_init_file(struct io_kiocb *req, fmode_t mode) {
	struct kiocb *kiocb = &req->rw.kiocb;
	struct io_ring_ctx *ctx = req->ctx;
	struct file *file = req->file;
	int ret;

	if (unlikely(!file || !(file->f_mode & mode)))
		return -EBADF;

	if (!io_req_ffs_set(req))
		req->flags |= io_file_get_flags(file) << REQ_F_SUPPORT_NOWAIT_BIT;

	kiocb->ki_flags = iocb_flags(file);
	ret = kiocb_set_rw_flags(kiocb, req->rw.flags);
	if (unlikely(ret))
		return ret;

	if ((kiocb->ki_flags & IOCB_NOWAIT) ||
	    ((file->f_flags & O_NONBLOCK) && !io_file_supports_nowait(req)))
		req->flags |= REQ_F_NOWAIT;

	if (ctx->flags & IORING_SETUP_IOPOLL) {
		if (!(kiocb->ki_flags & IOCB_DIRECT) || !file->f_op->iopoll)
			return -EOPNOTSUPP;

		kiocb->ki_flags |= IOCB_HIPRI | IOCB_ALLOC_CACHE;
		kiocb->ki_complete = io_complete_rw_iopoll;
		req->iopoll_completed = 0;
	} else {
		if (kiocb->ki_flags & IOCB_HIPRI)
			return -EINVAL;
		kiocb->ki_complete = io_complete_rw;
	}

	return 0;
}


static int io_read(struct io_kiocb *req, unsigned int issue_flags){
	struct io_rw_state __s, *s = &__s;
	struct iovec *iovec;
	struct kiocb *kiocb = &req->rw.kiocb;
	bool force_nonblock = issue_flags & IO_URING_F_NONBLOCK;
	struct io_async_rw *rw;
	ssize_t ret, ret2;
	loff_t *ppos;

	if (!req_has_async_data(req)) {
		ret = io_import_iovec(READ, req, &iovec, s, issue_flags);
		if (unlikely(ret < 0))
			return ret;
	} else {
		if (req->flags & REQ_F_BUFFER_SELECT) {
			ret = io_import_iovec(READ, req, &iovec, s, issue_flags);
			if (unlikely(ret < 0))
				return ret;
		}

		rw = req->async_data;
		s = &rw->s;
		iov_iter_restore(&s->iter, &s->iter_state);
		iovec = NULL;
	}
	ret = io_rw_init_file(req, FMODE_READ);
	if (unlikely(ret)) {
		kfree(iovec);
		return ret;
	}
	req->result = iov_iter_count(&s->iter);

	if (force_nonblock) {
		if (unlikely(!io_file_supports_nowait(req))) {
			ret = io_setup_async_rw(req, iovec, s, true);
			return ret ?: -EAGAIN;
		}
		kiocb->ki_flags |= IOCB_NOWAIT;
	} else {
		kiocb->ki_flags &= ~IOCB_NOWAIT;
	}

	ppos = io_kiocb_update_pos(req);

	ret = rw_verify_area(READ, req->file, ppos, req->result);
	if (unlikely(ret)) {
		kfree(iovec);
		return ret;
	}

	ret = io_iter_do_read(req, &s->iter);

	if (ret == -EAGAIN || (req->flags & REQ_F_REISSUE)) {
		req->flags &= ~REQ_F_REISSUE;
		if (req->opcode == IORING_OP_READ && file_can_poll(req->file))
			return -EAGAIN;
		if (!force_nonblock && !(req->ctx->flags & IORING_SETUP_IOPOLL))
			goto done;
		if (req->flags & REQ_F_NOWAIT)
			goto done;
		ret = 0;
	} else if (ret == -EIOCBQUEUED) {
		goto out_free;
	} else if (ret == req->result || ret <= 0 || !force_nonblock ||
		   (req->flags & REQ_F_NOWAIT) || !need_read_all(req)) {

		goto done;
	}

	iov_iter_restore(&s->iter, &s->iter_state);

	ret2 = io_setup_async_rw(req, iovec, s, true);
	if (ret2)
		return ret2;

	iovec = NULL;
	rw = req->async_data;
	s = &rw->s;

	do {
		iov_iter_advance(&s->iter, ret);
		if (!iov_iter_count(&s->iter))
			break;
		rw->bytes_done += ret;
		iov_iter_save_state(&s->iter, &s->iter_state);

		if (!io_rw_should_retry(req)) {
			kiocb->ki_flags &= ~IOCB_WAITQ;
			return -EAGAIN;
		}

		ret = io_iter_do_read(req, &s->iter);
		if (ret == -EIOCBQUEUED)
			return 0;
		kiocb->ki_flags &= ~IOCB_WAITQ;
		iov_iter_restore(&s->iter, &s->iter_state);
	} while (ret > 0);
done:
	kiocb_done(req, ret, issue_flags);
out_free:
	if (iovec)
		kfree(iovec);
	return 0;
}


//////////////////////////////////////////////////////////////////////
//XENO: Part of the path where uninitialized access occurs eventually
//////////////////////////////////////////////////////////////////////


static int io_do_iopoll(struct io_ring_ctx *ctx, bool force_nonspin) {
	struct io_wq_work_node *pos, *start, *prev;
	unsigned int poll_flags = BLK_POLL_NOSLEEP;
	DEFINE_IO_COMP_BATCH(iob);
	int nr_events = 0;

	if (ctx->poll_multi_queue || force_nonspin)
		poll_flags |= BLK_POLL_ONESHOT;

	wq_list_for_each(pos, start, &ctx->iopoll_list) {
		struct io_kiocb *req = container_of(pos, struct io_kiocb, comp_list);
		struct kiocb *kiocb = &req->rw.kiocb;
		int ret;

		if (READ_ONCE(req->iopoll_completed))
			break;

		ret = kiocb->ki_filp->f_op->iopoll(kiocb, &iob, poll_flags); //XENO: This calls iocb_bio_iopoll
		if (unlikely(ret < 0))
			return ret;
		else if (ret)
			poll_flags |= BLK_POLL_ONESHOT;

		if (!rq_list_empty(iob.req_list) ||
		    READ_ONCE(req->iopoll_completed))
			break;
	}

	if (!rq_list_empty(iob.req_list))
		iob.complete(&iob);
	else if (!pos)
		return 0;

	prev = start;
	wq_list_for_each_resume(pos, prev) {
		struct io_kiocb *req = container_of(pos, struct io_kiocb, comp_list);

		if (!smp_load_acquire(&req->iopoll_completed))
			break;
		nr_events++;
		if (unlikely(req->flags & REQ_F_CQE_SKIP))
			continue;
		__io_fill_cqe_req(req, req->result, io_put_kbuf(req, 0));
	}

	if (unlikely(!nr_events))
		return 0;

	io_commit_cqring(ctx);
	io_cqring_ev_posted_iopoll(ctx);
	pos = start ? start->next : ctx->iopoll_list.first;
	wq_list_cut(&ctx->iopoll_list, prev, start);
	io_free_batch_list(ctx, pos);
	return nr_events;
}


int iocb_bio_iopoll(struct kiocb *kiocb, struct io_comp_batch *iob, unsigned int flags) {
	struct bio *bio;
	int ret = 0;

	rcu_read_lock();
	bio = READ_ONCE(kiocb->private);
	if (bio && bio->bi_bdev)
		ret = bio_poll(bio, iob, flags);
	rcu_read_unlock();

	return ret;
}

int bio_poll(struct bio *bio, struct io_comp_batch *iob, unsigned int flags) {
	struct request_queue *q = bdev_get_queue(bio->bi_bdev);
	blk_qc_t cookie = READ_ONCE(bio->bi_cookie);
	int ret = 0;

	if (cookie == BLK_QC_T_NONE ||
	    !test_bit(QUEUE_FLAG_POLL, &q->queue_flags))
		return 0;

	blk_flush_plug(current->plug, false);

	if (blk_queue_enter(q, BLK_MQ_REQ_NOWAIT))
		return 0;
	if (queue_is_mq(q)) {
		ret = blk_mq_poll(q, cookie, iob, flags);
	} else {
		struct gendisk *disk = q->disk;

		if (disk && disk->fops->poll_bio)
			ret = disk->fops->poll_bio(bio, iob, flags);
	}
	blk_queue_exit(q);
	return ret;
}
```

Check that the code that attempts to initialize *struct kiocb *kiocb* struct by the function *io_rw_init_file()*, and later in *iocb_bio_iopoll()* attempts to use *kiocb->private* field which is uninitialize, thus *bio* now has uninitilize data and barely all the operations performed with *bio* in *bio_poll() *are dangerous and potentially vulnerable since the code is making decisions with undefined data.

<br>

